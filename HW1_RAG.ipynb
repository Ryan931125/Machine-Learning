{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdd2d6c",
   "metadata": {
    "_cell_guid": "612183c1-2351-4e73-9d9a-9b5ff7ed5822",
    "_uuid": "ca1cd241-2d72-4d4e-8e05-1234e4820c97",
    "collapsed": false,
    "id": "1TFwaJir_Olj",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.00675,
     "end_time": "2025-04-13T06:58:42.633646",
     "exception": false,
     "start_time": "2025-04-13T06:58:42.626896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f72b3",
   "metadata": {
    "_cell_guid": "6b5372ad-306a-4b76-8226-beea92b7938c",
    "_uuid": "801030f0-3b40-4227-b1ad-2b7fa39ffe51",
    "collapsed": false,
    "id": "6tQHdH2k_Olk",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005442,
     "end_time": "2025-04-13T06:58:42.645147",
     "exception": false,
     "start_time": "2025-04-13T06:58:42.639705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4951b",
   "metadata": {
    "_cell_guid": "148b6966-3ead-4326-8387-eed7609de187",
    "_uuid": "1085bb20-4827-41ad-bf86-b3dff3513419",
    "collapsed": false,
    "id": "mGx000oZ_Oll",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.005708,
     "end_time": "2025-04-13T06:58:42.657204",
     "exception": false,
     "start_time": "2025-04-13T06:58:42.651496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b4287ae",
   "metadata": {
    "_cell_guid": "d0190351-7134-498e-adb2-47ab50a83d2e",
    "_uuid": "caed96fd-e3b7-42e7-8085-9156b9e147a8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T06:58:42.670136Z",
     "iopub.status.busy": "2025-04-13T06:58:42.669785Z",
     "iopub.status.idle": "2025-04-13T07:00:03.509051Z",
     "shell.execute_reply": "2025-04-13T07:00:03.507924Z"
    },
    "id": "5JywoPOO_Oll",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 80.847807,
     "end_time": "2025-04-13T07:00:03.510886",
     "exception": false,
     "start_time": "2025-04-13T06:58:42.663079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\r\n",
      "Collecting llama-cpp-python==0.3.4\r\n",
      "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl (445.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m163.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\r\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\r\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\r\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (3.1.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\r\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\r\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\r\n",
      "Collecting googlesearch-python\r\n",
      "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Collecting bs4\r\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\r\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.4.1)\r\n",
      "Collecting requests-html\r\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting lxml_html_clean\r\n",
      "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.12.3)\r\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.32.3)\r\n",
      "Collecting pyquery (from requests-html)\r\n",
      "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Collecting fake-useragent (from requests-html)\r\n",
      "  Downloading fake_useragent-2.1.0-py3-none-any.whl.metadata (17 kB)\r\n",
      "Collecting parse (from requests-html)\r\n",
      "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting w3lib (from requests-html)\r\n",
      "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\r\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\r\n",
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\r\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.5.0)\r\n",
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\r\n",
      "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\r\n",
      "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\r\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\r\n",
      "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\r\n",
      "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\r\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\r\n",
      "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\r\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\r\n",
      "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\r\n",
      "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fake_useragent-2.1.0-py3-none-any.whl (125 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\r\n",
      "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\r\n",
      "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\r\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\r\n",
      "  Attempting uninstall: websockets\r\n",
      "    Found existing installation: websockets 14.1\r\n",
      "    Uninstalling websockets-14.1:\r\n",
      "      Successfully uninstalled websockets-14.1\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 2.3.0\r\n",
      "    Uninstalling urllib3-2.3.0:\r\n",
      "      Successfully uninstalled urllib3-2.3.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "google-genai 0.2.2 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.1.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.2 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\r\n",
      "--2025-04-13 06:59:15--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\r\n",
      "Resolving huggingface.co (huggingface.co)... 18.244.202.73, 18.244.202.118, 18.244.202.60, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|18.244.202.73|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250413%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250413T065915Z&X-Amz-Expires=3600&X-Amz-Signature=a1076fbe41b2343cc69a4f1fa09257d2f52dabd62bb7627d2829e738bc5b594a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1744531155&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDUzMTE1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=Re0lhkOC9HqoFweNtS6Hv6C5iutMOqrkZ04BpQkMtPO6VwR8qeZWX14rYZgzd0NSMEXb7-JBHwn%7ElDFPHlNKju7mDV9isvkT6JHUnw2rTGsp1aYP1AyvnS8stcO6wl8C6FJa-gfKH1LNNNZdajHnfpisiazRxG90ms59l1yb%7EXHqBO-Gd9FUQRliYCBUjfzUc0xx-Nkpe1wPRdnUFUqiQg%7EeBwUlPc%7E2rWRGoXrH-e8KjD6i9h2XFotLhPdWOSzMulTntLE5KX%7E1YosZBuTn07CuDQyD4d0mnR-mziYG2MWEUbihy%7ETnpEtxNxc5hdijc0mPztlJaCH-nI0rZRl5YA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\r\n",
      "--2025-04-13 06:59:15--  https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250413%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250413T065915Z&X-Amz-Expires=3600&X-Amz-Signature=a1076fbe41b2343cc69a4f1fa09257d2f52dabd62bb7627d2829e738bc5b594a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1744531155&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDUzMTE1NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=Re0lhkOC9HqoFweNtS6Hv6C5iutMOqrkZ04BpQkMtPO6VwR8qeZWX14rYZgzd0NSMEXb7-JBHwn%7ElDFPHlNKju7mDV9isvkT6JHUnw2rTGsp1aYP1AyvnS8stcO6wl8C6FJa-gfKH1LNNNZdajHnfpisiazRxG90ms59l1yb%7EXHqBO-Gd9FUQRliYCBUjfzUc0xx-Nkpe1wPRdnUFUqiQg%7EeBwUlPc%7E2rWRGoXrH-e8KjD6i9h2XFotLhPdWOSzMulTntLE5KX%7E1YosZBuTn07CuDQyD4d0mnR-mziYG2MWEUbihy%7ETnpEtxNxc5hdijc0mPztlJaCH-nI0rZRl5YA__&Key-Pair-Id=K2L8F4GPSG1IFC\r\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.155.1.18, 18.155.1.105, 18.155.1.89, ...\r\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.155.1.18|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 8540775840 (8.0G)\r\n",
      "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\r\n",
      "\r\n",
      "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G   244MB/s    in 38s     \r\n",
      "\r\n",
      "2025-04-13 06:59:53 (216 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\r\n",
      "\r\n",
      "--2025-04-13 06:59:53--  https://www.csie.ntu.edu.tw/~ulin/public.txt\r\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\r\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 4399 (4.3K) [text/plain]\r\n",
      "Saving to: ‘public.txt’\r\n",
      "\r\n",
      "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2025-04-13 06:59:54 (126 MB/s) - ‘public.txt’ saved [4399/4399]\r\n",
      "\r\n",
      "--2025-04-13 07:00:01--  https://www.csie.ntu.edu.tw/~ulin/private.txt\r\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\r\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 15229 (15K) [text/plain]\r\n",
      "Saving to: ‘private.txt’\r\n",
      "\r\n",
      "private.txt         100%[===================>]  14.87K  76.2KB/s    in 0.2s    \r\n",
      "\r\n",
      "2025-04-13 07:00:03 (76.2 KB/s) - ‘private.txt’ saved [15229/15229]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
    "\n",
    "from pathlib import Path\n",
    "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
    "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
    "if not Path('./public.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
    "if not Path('./private.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184c3372",
   "metadata": {
    "_cell_guid": "5386a11c-4801-4cff-b71f-f7d277456026",
    "_uuid": "985e0103-9f49-4901-800b-177d3cf8264a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:03.557051Z",
     "iopub.status.busy": "2025-04-13T07:00:03.556660Z",
     "iopub.status.idle": "2025-04-13T07:00:07.043490Z",
     "shell.execute_reply": "2025-04-13T07:00:07.042352Z"
    },
    "id": "kX6SizAt_Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 3.511407,
     "end_time": "2025-04-13T07:00:07.044989",
     "exception": false,
     "start_time": "2025-04-13T07:00:03.533582",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
    "else:\n",
    "    print('You are good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32e752",
   "metadata": {
    "_cell_guid": "7355cce3-30db-4bf5-ad7c-38aeca0c4e59",
    "_uuid": "028c3e47-a64e-4987-b762-66c3e75194f0",
    "collapsed": false,
    "id": "l3iyc1qC_Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021654,
     "end_time": "2025-04-13T07:00:07.087930",
     "exception": false,
     "start_time": "2025-04-13T07:00:07.066276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prepare the LLM and LLM utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9986d",
   "metadata": {
    "_cell_guid": "eee3cec5-659e-4919-a394-e758dd3eb1eb",
    "_uuid": "b281566e-7926-498d-a3a6-db072be3eaa4",
    "collapsed": false,
    "id": "T59vxAo2_Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021779,
     "end_time": "2025-04-13T07:00:07.130823",
     "exception": false,
     "start_time": "2025-04-13T07:00:07.109044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea42873",
   "metadata": {
    "_cell_guid": "5a750e51-8cab-457d-bad6-1c1b816b4996",
    "_uuid": "102071be-47b6-4039-88b8-b1cd14205afb",
    "collapsed": false,
    "id": "vtepTeT3_Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022364,
     "end_time": "2025-04-13T07:00:07.175782",
     "exception": false,
     "start_time": "2025-04-13T07:00:07.153418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
    "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e19139",
   "metadata": {
    "_cell_guid": "9fac982d-202f-417b-978d-056253321779",
    "_uuid": "cf7f0029-7db3-41a0-a22d-0c21add26565",
    "collapsed": false,
    "id": "eVil2Vhe_Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022416,
     "end_time": "2025-04-13T07:00:07.220700",
     "exception": false,
     "start_time": "2025-04-13T07:00:07.198284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90899e1",
   "metadata": {
    "_cell_guid": "9c78fb01-6f5a-4672-b387-fe3b33ab2056",
    "_uuid": "f2448490-2292-4997-9cff-ca640750e0fc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:07.267488Z",
     "iopub.status.busy": "2025-04-13T07:00:07.266981Z",
     "iopub.status.idle": "2025-04-13T07:00:11.622959Z",
     "shell.execute_reply": "2025-04-13T07:00:11.621997Z"
    },
    "id": "ScyW45N__Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 4.381463,
     "end_time": "2025-04-13T07:00:11.624837",
     "exception": false,
     "start_time": "2025-04-13T07:00:07.243374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model onto GPU\n",
    "llama3 = Llama(\n",
    "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
    ")\n",
    "\n",
    "def generate_response(_model: Llama, _messages: str) -> str:\n",
    "    '''\n",
    "    This function will inference the model with given messages.\n",
    "    '''\n",
    "    _output = _model.create_chat_completion(\n",
    "        _messages,\n",
    "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
    "        max_tokens=512,    # This argument is how many tokens the model can generate.\n",
    "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
    "        repeat_penalty=2.0,\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return _output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376d3c8",
   "metadata": {
    "_cell_guid": "bb9d1551-4679-4ef0-97f2-4aff29943f89",
    "_uuid": "2e62a23c-0e5b-4971-9f5f-52f30df32f27",
    "collapsed": false,
    "id": "tnHLwq-4_Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023023,
     "end_time": "2025-04-13T07:00:11.671307",
     "exception": false,
     "start_time": "2025-04-13T07:00:11.648284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54146386",
   "metadata": {
    "_cell_guid": "4432c706-a0bd-4928-b02a-4258d6f9f26f",
    "_uuid": "c6add9d5-2ded-4892-ac04-932a8eec786c",
    "collapsed": false,
    "id": "SYM-2ZsE_Olm",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022666,
     "end_time": "2025-04-13T07:00:11.716537",
     "exception": false,
     "start_time": "2025-04-13T07:00:11.693871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b142dd",
   "metadata": {
    "_cell_guid": "519c68c7-df9f-40dc-b681-00b1dcb002e4",
    "_uuid": "a3a95306-bc1e-4a54-af38-30098ad9062d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:11.763678Z",
     "iopub.status.busy": "2025-04-13T07:00:11.763190Z",
     "iopub.status.idle": "2025-04-13T07:00:12.240943Z",
     "shell.execute_reply": "2025-04-13T07:00:12.239963Z"
    },
    "id": "bEIRmZl7_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.503666,
     "end_time": "2025-04-13T07:00:12.242796",
     "exception": false,
     "start_time": "2025-04-13T07:00:11.739130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import detect\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    results = await get_htmls(results)\n",
    "    # Filter out the None values.\n",
    "    results = [x for x in results if x is not None]\n",
    "    # Parse the HTML.\n",
    "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
    "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
    "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
    "    # Return the first n results.\n",
    "    return results[:n_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe0cdd",
   "metadata": {
    "_cell_guid": "88c204d8-ee80-4cac-bf42-d3938d28d042",
    "_uuid": "53c1ecba-65c3-4e02-b583-d140ea69278f",
    "collapsed": false,
    "id": "rC3zQjjj_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022155,
     "end_time": "2025-04-13T07:00:12.287792",
     "exception": false,
     "start_time": "2025-04-13T07:00:12.265637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test the LLM inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e8832e4",
   "metadata": {
    "_cell_guid": "1fd8e4d0-0d37-44dd-9984-3a89845e72b3",
    "_uuid": "9aa4e707-9dbb-43cd-bbfd-ad31dbca66f0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:12.333907Z",
     "iopub.status.busy": "2025-04-13T07:00:12.333307Z",
     "iopub.status.idle": "2025-04-13T07:00:22.391816Z",
     "shell.execute_reply": "2025-04-13T07:00:22.390732Z"
    },
    "id": "8dmGCARd_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 10.083266,
     "end_time": "2025-04-13T07:00:22.393318",
     "exception": false,
     "start_time": "2025-04-13T07:00:12.310052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是 LLaMA-3.1，雖然我的訓練資料包含了大量的英文文本，但在中文方面，我更為熟悉和流暢。因爲大部分的人使用繁體或簡化字寫作，而這些都是用來進行模型學習與測試用的。\n",
      "\n",
      "我可以理解、生成以及回答以中華民國（台灣）地區通行的標準正確且完整中文文本，包括但不限於：\n",
      "\n",
      "*  正式文件\n",
      "    *   政府公報及法令草案等政府機關出版物。\n",
      "        +     《憲政體制改革方案》\n",
      "            -      中華民國總統會同行政院頒布之《中正紀念堂管理條例》修訂稿（2023年6月30日起施行）\n",
      "*  文學作品\n",
      "    *   小說、散文及詩歌等。\n",
      "        +     《西遊記》\n",
      "            -      書名：水滴石穿；原作《三國演義》，作者吳承恩，改編自唐代小説家施耐庵的同題著述\n",
      "*  日常對話與書信\n",
      "\n",
      "但請注意，我可能會因爲訓練資料或語言限制而無法完全理解某些特殊用詞、術语等。\n"
     ]
    }
   ],
   "source": [
    "# You can try out different questions here.\n",
    "test_question='Are you more fluent in english or chinese'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。回答內容請盡量詳細。\"},    # System prompt\n",
    "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
    "]\n",
    "\n",
    "print(generate_response(llama3, messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2858c572",
   "metadata": {
    "_cell_guid": "a1a47fb7-7f8d-410f-81d7-fdbc4fd36f7a",
    "_uuid": "20f06ec9-c03a-46fa-8da8-c34edc9a7dd9",
    "collapsed": false,
    "id": "C0-ojJuE_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021992,
     "end_time": "2025-04-13T07:00:22.438083",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.416091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72a49f3",
   "metadata": {
    "_cell_guid": "47dc2152-16af-48c3-831d-9b0a120ad012",
    "_uuid": "053a07e9-85e2-4cf5-92ac-900763693ad1",
    "collapsed": false,
    "id": "HGsIPud3_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022879,
     "end_time": "2025-04-13T07:00:22.483564",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.460685",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
    "- Attributes:\n",
    "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
    "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
    "    - llm: Just an indicator of the LLM model used by the agent.\n",
    "- Method:\n",
    "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5487c762",
   "metadata": {
    "_cell_guid": "01d388cb-e56d-4a6c-8edd-f26b9367f71c",
    "_uuid": "995b30b0-3308-44bb-975f-2da540d51be8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:22.530418Z",
     "iopub.status.busy": "2025-04-13T07:00:22.530071Z",
     "iopub.status.idle": "2025-04-13T07:00:22.535709Z",
     "shell.execute_reply": "2025-04-13T07:00:22.534756Z"
    },
    "id": "zjG-UwDX_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.030996,
     "end_time": "2025-04-13T07:00:22.537279",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.506283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LLMAgent():\n",
    "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
    "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
    "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
    "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
    "    def inference(self, message:str) -> str:\n",
    "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
    "            # TODO: Design the system prompt and user prompt here.\n",
    "            # Format the messsages first.\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
    "                {\"role\": \"user\", \"content\": f\"你要做的事：\\n{self.task_description}\\n\\n以下是輸入：\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
    "            ]\n",
    "            return generate_response(llama3, messages)\n",
    "        else:\n",
    "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f9ca7",
   "metadata": {
    "_cell_guid": "5bd05330-bca6-4518-941d-00da0a095aa6",
    "_uuid": "7f5a75ad-cab2-4283-8ccf-a611573234dd",
    "collapsed": false,
    "id": "0-ueJrgP_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022413,
     "end_time": "2025-04-13T07:00:22.582853",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.560440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TODO 1: Design the role description and task description for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5278125d",
   "metadata": {
    "_cell_guid": "07436aef-7b73-435c-924f-81fd3cc0a15d",
    "_uuid": "151201dc-4bdc-4ff2-9911-c9bf3d461ea6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:22.684173Z",
     "iopub.status.busy": "2025-04-13T07:00:22.683860Z",
     "iopub.status.idle": "2025-04-13T07:00:22.688688Z",
     "shell.execute_reply": "2025-04-13T07:00:22.687807Z"
    },
    "id": "DzPzmNnj_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.084224,
     "end_time": "2025-04-13T07:00:22.690224",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.606000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Design the role and task description for each agent.\n",
    "\n",
    "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
    "# Works fine not exactly 100% accurate\n",
    "question_extraction_agent = LLMAgent( \n",
    "    role_description=\"你是一個問題提取專家。使用中文時只會使用繁體中文來回問題。\",\n",
    "    task_description=\"從輸入中擷取完整問句。完整保留問句的所有資訊，確保它仍然表達原本的意思，不刪除重要細節。刪除問題前的背景敘述與無關內容，但不能刪除影響問題理解的上下文。若輸入本身是問句，直接輸出。若輸入是簡單問題也直接輸出。你應該輸出一個完整的問句。僅輸出問句，不回答。使用繁體中文。\",\n",
    ")\n",
    "\n",
    "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
    "# Works fine not exactly 100% accurate 還可優化\n",
    "keyword_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是一個關鍵詞提取專家。使用中文時只會用繁體中文回答問題。\",\n",
    "    task_description=\"你會仔細思考後從輸入提取問題中的關鍵詞。輸出只有你提取的關鍵詞，以空白格開。輸入可能是背景敘述和一個問題，或僅一個問題，你要找的是與該問題相關的關鍵詞。關鍵詞可能是專有名詞（如：人名、地名、機構名稱、技術名詞）、問題的主詞動詞受詞、還有疑問詞（什麼名字、時間、地點）。你輸出的關鍵字都應該在問題中，不要嘗試回答問題，也不用澄清想法。使用繁體中文。\",\n",
    ")\n",
    "\n",
    "# This agent is the core component that answers the question.\n",
    "qa_agent = LLMAgent(\n",
    "    role_description=\"你是一個回答問題的天才。你回答時只會用簡答。使用中文時只會使用繁體中文回答問題。\",\n",
    "    task_description=\"你要回答一個問題。後面有附上參考資料，你會讀完後一步一步思考，謹慎的回答。一率回答簡答，不用完整句子、不用解釋、也不用重述問題。使用繁體中文。你能不能正確回答嚴重影響我的職業生崖。\",\n",
    ")\n",
    "\n",
    "fact_check_guy = LLMAgent(\n",
    "    role_description=\"你是一個事實查核專家。使用中文時只會用繁體中文回答問題。\",\n",
    "    task_description=\"你要負責檢查一個回問題有沒有被答對，如果答對就輸出原本的答案，答錯的話輸出問題的正確答案，不用特別註答對或答錯。後面有附上參考資料，你會讀完後一步一步思考，謹慎的回答。一率回答簡答，不用完整句子、不用解釋、也不用重述問題。使用繁體中文。你如果答錯會受到極爲嚴重的懲罰。\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b0afa",
   "metadata": {
    "_cell_guid": "865a5851-77fa-45b0-b6da-f68eaf3d6725",
    "_uuid": "240b5813-3c76-402c-95f8-b1c503d38c3d",
    "collapsed": false,
    "id": "A9eoywr7_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023227,
     "end_time": "2025-04-13T07:00:22.737429",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.714202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90e5db",
   "metadata": {
    "_cell_guid": "e67a02f4-0266-4664-acdc-fc26f9dcbb3d",
    "_uuid": "75e23781-9a34-4037-90a2-e3c00447f4eb",
    "collapsed": false,
    "id": "8HDOjNYJ_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025217,
     "end_time": "2025-04-13T07:00:22.785804",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.760587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "TODO 2: Implement the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94665989",
   "metadata": {
    "_cell_guid": "d28dc00b-0364-47be-a34b-5fdb830ac7b5",
    "_uuid": "a61d0a51-4c89-49c2-af5c-da423e1860fb",
    "collapsed": false,
    "id": "MRGNa-1i_Oln",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023086,
     "end_time": "2025-04-13T07:00:22.833230",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.810144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Please refer to the homework description slides for hints.\n",
    "\n",
    "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd9f81",
   "metadata": {
    "_cell_guid": "32ba74b6-773a-4955-95e1-5598c51be0fb",
    "_uuid": "908b5779-e06c-4694-a4ed-2db7208dd009",
    "collapsed": false,
    "id": "cMaIsKAZ_Olo",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023857,
     "end_time": "2025-04-13T07:00:22.878855",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.854998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Naive approach (simple baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f9ac1f",
   "metadata": {
    "_cell_guid": "a19bdfa0-67cf-4e7d-a2b5-e28071e78eeb",
    "_uuid": "87eb54dd-7de3-4201-8140-e198163b86d7",
    "collapsed": false,
    "id": "mppO-oOO_Olo",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.021398,
     "end_time": "2025-04-13T07:00:22.922453",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.901055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Naive RAG approach (medium baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36ad2a",
   "metadata": {
    "_cell_guid": "626d7df8-786e-439c-87ec-58076ca21832",
    "_uuid": "a3674077-8b58-43b8-a7f3-e0a659344731",
    "collapsed": false,
    "id": "HYxbciLO_Olo",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022918,
     "end_time": "2025-04-13T07:00:22.968021",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.945103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- RAG with agents (strong baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57175a5",
   "metadata": {
    "_cell_guid": "7203959c-7c27-4e12-b686-27064487b1fb",
    "_uuid": "5b5bbb8d-65d7-4c45-87cb-dfabe8ff59cd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:23.016423Z",
     "iopub.status.busy": "2025-04-13T07:00:23.016041Z",
     "iopub.status.idle": "2025-04-13T07:00:23.021674Z",
     "shell.execute_reply": "2025-04-13T07:00:23.020857Z"
    },
    "id": "ztJkA7R7_Olo",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.031497,
     "end_time": "2025-04-13T07:00:23.023098",
     "exception": false,
     "start_time": "2025-04-13T07:00:22.991601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    # TODO: Implement your pipeline.\n",
    "    # Currently, it only feeds the question directly to the LLM.\n",
    "    # You may want to get the final results through multiple inferences.\n",
    "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
    "\n",
    "    # return question_extraction_agent.inference(question)\n",
    "    \n",
    "    # return keyword_extraction_agent.inference(question)\n",
    "\n",
    "    extracted_question = question_extraction_agent.inference(question)\n",
    "\n",
    "    keywords = keyword_extraction_agent.inference(question)\n",
    "\n",
    "    # Get search results from the internet\n",
    "    search_results = await search(keywords)\n",
    "    \n",
    "    # Append the search results to the question\n",
    "    full_input = \"請回答這個問題:\" + extracted_question + \"\\n網路上的相關資料：\" + \"\\n\".join(search_results)\n",
    "\n",
    "    # Tokenize by character (each letter is a token)\n",
    "    tokens = list(full_input)  # Convert string into a list of characters\n",
    "\n",
    "    # Truncate to 16,370 tokens (characters)\n",
    "    truncated_tokens = tokens[:16370]\n",
    "\n",
    "    # Reconstruct the truncated text\n",
    "    truncated_input = \"\".join(truncated_tokens)\n",
    "\n",
    "    # Send truncated input to the model\n",
    "    raw_answer =  qa_agent.inference(truncated_input)\n",
    "\n",
    "    second_input = \"問題：\" + extracted_question + \"\\n回答：\" + raw_answer + \"\\n網路上的相關資料：\" + \"\\n\".join(search_results)\n",
    "    new_tokens = list(second_input)\n",
    "    trunc_tokens = new_tokens[:16370]\n",
    "    final_input = \"\".join(trunc_tokens)\n",
    "\n",
    "    return fact_check_guy.inference(final_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec0acd",
   "metadata": {
    "_cell_guid": "af97a933-19f2-4ee2-a896-6f3394700123",
    "_uuid": "d63a229d-9d4d-4f7b-a5b5-5576aa37aa53",
    "collapsed": false,
    "id": "P_kI_9EGB0S9",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022679,
     "end_time": "2025-04-13T07:00:23.068885",
     "exception": false,
     "start_time": "2025-04-13T07:00:23.046206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Answer the questions using your pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fcb232",
   "metadata": {
    "_cell_guid": "3d2d6621-b8b4-488f-b180-6e3ac1d0788e",
    "_uuid": "2e44e856-c6b8-4953-b17f-26d5c07c6328",
    "collapsed": false,
    "id": "PN17sSZ8DUg7",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023082,
     "end_time": "2025-04-13T07:00:23.114344",
     "exception": false,
     "start_time": "2025-04-13T07:00:23.091262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8838efbe",
   "metadata": {
    "_cell_guid": "38b4ad99-fbaf-4c7a-a313-35a03a322328",
    "_uuid": "091cace4-bbc9-4795-9549-10874164914a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T07:00:23.161261Z",
     "iopub.status.busy": "2025-04-13T07:00:23.160960Z",
     "iopub.status.idle": "2025-04-13T08:23:49.829562Z",
     "shell.execute_reply": "2025-04-13T08:23:49.828454Z"
    },
    "id": "plUDRTi_B39S",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5006.720158,
     "end_time": "2025-04-13T08:23:49.857114",
     "exception": false,
     "start_time": "2025-04-13T07:00:23.136956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 國立臺灣師範大學\n",
      "2 750元\n",
      "3 史蒂夫·乔布斯\n",
      "4 80\n",
      "5 觸地得分\n",
      "6 該地位於現今的臺東縣。\n",
      "7 熊仔的藝名叫做「貓頭鷹」\n",
      "8 迈克尔·法拉第\n",
      "9 康樂車站\n",
      "10 自己練\n",
      "11 洛杉磯湖人\n",
      "12 唐纳·川普\n",
      "13 1B\n",
      "14 停修的限制是：若學生申請停止上課，至少要在期末考或報告繳交前提出需求。\n",
      "15 DeepSeek公司的母 公司是幻方量化。\n",
      "16 波士顿凯尔特人\n",
      "17 會出現異常反應\n",
      "18 艾伦·图灵\n",
      "19 真武大帝\n",
      "20 Windows 作業系統是微軟公司的產品。\n",
      "21 新北市的地藏庵\n",
      "22 《咒》的邪神名為：無明\n",
      "23 徐志摩\n",
      "24 利嘉部落\n",
      "25 GeForce RTX 40系列\n",
      "26 日本\n",
      "27 艾萨克·牛顿\n",
      "28 TAIHUCAIS\n",
      "29 《终极警探》\n",
      "30 水的化學式為H2O。\n",
      "31 第15個作業是什麼？  生成式人工智慧導論課程的Homework 10\n",
      "32 國防醫學大學\n",
      "33 BT协议的机制是通过种子文件（.torrent）来确保当一个新的节点加入网络时，也能从其他seed随機地获得部分数据，以利于后续整个网絡中的資料交換。\n",
      "34 你要去哪裡？\n",
      "35 ç½è\n",
      "36 嘟胖\n",
      "37 國立臺灣大學物理治療學系的正常修業年限為六個月。\n",
      "38 ç½ä¸æï¼å¯ã\n",
      "39 是伊達政宗\n",
      "40 王肥貓同學最有可能去修的課程是數位素養導航NavigationintoDigitalLiteracy。\n",
      "41 18個國家或地區\n",
      "42 馬智禮\n",
      "43 片頭曲是《Killkiss》。\n",
      "44 1991\n",
      "45 利卡文\n",
      "46 红茶是全发酵的。\n",
      "47 超魔導龍騎士-真紅眼黑龙骑兵\n",
      "48 豐田萌繪在《BanG Dream!》企劃中，擔任松原花音的聲優。\n",
      "49 罗纳尔多\n",
      "50 冥王星\n",
      "51 野生動物救傷單位位於宜蘭縣。  參考資料：政府資訊公開-法規资讯法律、 法规及行政规定公文函釋訴願決定施政計畫統计与出版品會議纪录預算與決 算書政策宣導執行情形支付或接受之補助会 计报告其他信息:::回首頁>政府資訊公開法規资讯法律、 法规及行政规定公文函釋訴願決定施政計畫統计与出版品會議纪录預算與決 算書政策宣導執行情形支付或接受之補助会 计报告其他信息:::\n",
      "52 是的，特有生物研究保育中心是一個很好的親子旅遊景點。\n",
      "53 DeSTA2\n",
      "54 太陽系中體積最大的行星是木衛一。\n",
      "55 達悟語\n",
      "56 老師是誰？  傅斯年\n",
      "57 阿美族\n",
      "58 布農族與鄒族自治部落息居相關。\n",
      "59 亚莉纳·可洛瓦（アリナ・クローバー）\n",
      "60 射馬干部落的創始祖是杜姑（Tuku），她後來各自建立了兩個不同的族群，分別為知本和卡砦 卡蘭。\n",
      "61 KO.1田弘光\n",
      "62 紅黑樹（rbtree）\n",
      "63 霸王行动（Operation Overlord）\n",
      "64 雙人格歌姬‧Aroma\n",
      "65 演說\n",
      "66 買不起輝達最新的 5090 顯卡，可能是因為價格太高。\n",
      "67 冠軍是中华台北\n",
      "68 《水滸傳》、《三國演義》， 《金瓶梅》（又名為西遊記）\n",
      "69 子时是23:00-01：59\n",
      "70 EXPIRE命令用于设置key的过期时间，单位是秒。例如：redis> EXIRE key 10  PEXIPERE 命令与expire类似，但其参数为毫米。  PERSIST可以移除一个键值对中的生存时长（也就是说这个KEY不会被自动删除）。  EXPIREAT命令用于设置key的过期时间，单位是秒。例如：redis> EXIPEAT key 10  PTTL 命令与 TTL 类似，但其参数为毫米。  TLL 和 PTIL 都可以用来获取一个键值对剩余生存时长（也就是距离这个KEY被自动删除还有多久）。\n",
      "71 C8763\n",
      "72 柴城位於現今的屏東縣車 城。\n",
      "73 Colab Pro\n",
      "74 國立台灣大學\n",
      "75 至少要學習 90 分才可以不用簽減免課程申請書。\n",
      "76 Neuro-sama 的最初的 Live2D 模型是使用 VTube Studio 預設角色 Momose Hiyori。\n",
      "77 尤里烏斯·優克歷亞\n",
      "78 紐開普市\n",
      "79 玉米是雙子的植物。\n",
      "80 陆军軍歌（中华民国）的前六字是：風雲起，山河動。\n",
      "81 物理、化學以及生物科目可以只擇一修習即可。\n",
      "82 憂傷湖（Lacus Doloris）、死lake （不確定是什麼意思） 、忘 lake  (同上) ，恐怖Lake   以及愛灣，以上五個地形位於月球背面的答案：無法確認。  正解: 憑藉網路上的相關資料，我發現憂傷湖（Lacus Doloris）、死lake （不確定是什麼意思） 、忘 lake  (同上) ，恐怖Lake   以及愛灣，以上五個地形位於月球背面。\n",
      "83 月光奏鸣曲\n",
      "84 舒米恩\n",
      "85 Doey the Doughman\n",
      "86 嘉義縣\n",
      "87 米開朗基羅的《大衛》雛型最初是在佛罗伦萨展現其早年的作品。  答案：是\n",
      "88 蔣中正\n",
      "89 台北暗殺星\n",
      "90 非莊家一開始的手牌有13張。\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fill in your student ID first.\n",
    "STUDENT_ID = \"b12902014\"\n",
    "\n",
    "STUDENT_ID = STUDENT_ID.lower()\n",
    "with open('./public.txt', 'r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    questions = [l.strip().split(',')[0] for l in questions]\n",
    "    for id, question in enumerate(questions, 1):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
    "            print(answer, file=output_f)\n",
    "\n",
    "with open('./private.txt', 'r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    for id, question in enumerate(questions, 31):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
    "            print(answer, file=output_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5318b1d5",
   "metadata": {
    "_cell_guid": "8e629e59-7e93-4678-aa28-c3585bd8f54b",
    "_uuid": "290fd559-706a-49ab-8891-880c1ed894cf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-13T08:23:49.914269Z",
     "iopub.status.busy": "2025-04-13T08:23:49.913889Z",
     "iopub.status.idle": "2025-04-13T08:23:49.922033Z",
     "shell.execute_reply": "2025-04-13T08:23:49.921055Z"
    },
    "id": "GmLO9PlmEBPn",
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.03865,
     "end_time": "2025-04-13T08:23:49.923788",
     "exception": false,
     "start_time": "2025-04-13T08:23:49.885138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine the results into one file.\n",
    "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
    "    for id in range(1,91):\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
    "            answer = input_f.readline().strip()\n",
    "            print(answer, file=output_f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5112.075631,
   "end_time": "2025-04-13T08:23:51.681507",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T06:58:39.605876",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
